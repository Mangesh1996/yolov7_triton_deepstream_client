# yolov7-triton-deepstream-client
```
change the input file in the run_docker.sh file.(support format  RTSP and MP4)
```

## Demo
Run demo:
```bash
bash run_docker.sh 
```
## NOTE
```
This is the only Yolov7-deeptstream-client inference on GRPC, you also need a Triton server to load the Yolov7 plan file (engine) model.
kindly refer.:-
```
<details><summary> <b>Expand</b> </summary>

* (https://github.com/WongKinYiu/yolov7)

* (https://github.com/triton-inference-server/server)
</details>



## Acknowledgements

<details><summary> <b>Expand</b> </summary>

* [https://github.com/WongKinYiu/yolov7](https://github.com/WongKinYiu/yolov7)
* [https://github.com/NVIDIA-AI-IOT/deepstream_python_apps](https://github.com/NVIDIA-AI-IOT/deepstream_python_apps)
* [https://github.com/thanhlnbka/yolov7-triton-deepstream/](https://github.com/thanhlnbka/yolov7-triton-deepstream)

</details>
